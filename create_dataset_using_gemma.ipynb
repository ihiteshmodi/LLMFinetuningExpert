{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703eb3e4-9c83-4dd7-a1d9-f71b1414a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ready!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4167fb6-3492-42d8-9387-c2da846db5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uninstall all modules if they exist\n",
    "\n",
    "!pip uninstall -y torch torchvision transformers accelerate huggingface_hub requests tqdm protobuf \\\n",
    "    langchain sentencepiece safetensors xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee232e2-6acf-46a6-87b2-9d921b2309da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install all compatible versions\n",
    "# PyTorch + TorchVision (CUDA 11.8 compatible with A40)\n",
    "!pip install --no-deps torch==2.1.0+cu118 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Transformers, Accelerate, HuggingFace Hub\n",
    "!pip install transformers==4.51.3\n",
    "!pip install accelerate==1.7.0\n",
    "!pip install huggingface_hub==0.32.4\n",
    "\n",
    "# Utility libraries\n",
    "!pip install requests==2.31.0\n",
    "!pip install tqdm==4.67.1\n",
    "!pip install protobuf==3.20.3\n",
    "\n",
    "# LangChain (optional, only if you need it)\n",
    "!pip install langchain==0.1.14\n",
    "\n",
    "# SentencePiece and Safetensors\n",
    "!pip install sentencepiece==0.2.0\n",
    "!pip install safetensors==0.5.3\n",
    "\n",
    "# xFormers (must match CUDA version & torch)\n",
    "!pip install xformers==0.0.29.post3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b1c59bd-364b-4f77-acb3-2a289741ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all required packages\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from langchain.schema import Document\n",
    "import torch\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "# Importing packages required to download model\n",
    "import requests\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "\n",
    "#For offloading model to GPU\n",
    "from accelerate import infer_auto_device_map, init_empty_weights\n",
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3c17dba-f46d-44a4-81ac-5cdc4a8c83f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb  all_marketing_material.pkl\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "853bf791-e534-42cf-a567-fea8b1592caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickle file to extract marketing docs from it\n",
    "pickle_file_path = 'all_marketing_material.pkl'\n",
    "\n",
    "# Load the data\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    all_marketing_pages = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93085593-e890-46f9-99a0-976438c4e90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into huggingface, as gemma model is closed\n",
    "secret_value_0 = \"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\"\n",
    "login(token=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086fde93-da08-4075-ab23-e8d401cccb49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733692c555dc4ea897a51dc959d287b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01f1cc8b7aa49b9be837f63ced8fcce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e7f71789e74153a715c0117d2a00bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3201da91044b44289c1c0ed7441251fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca89c5b984ca405bb3d63a20753676be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ea702aab7b472589bbba445d980af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c838556b7a3d4eb29563db4a7d4b1fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855d9c1b262240eba86562568308c52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454e090abda14fd69eec44eb13b6fcc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f039a0d330df4c98b7e31d6b53a6449b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d76c30ebf35445db7c265df29f59126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a851493a83b4491b9b8141b04f452d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5dcbe2be5c846dc844b4fc3f7199588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13447bc7ff4f490c8705ae617a2678a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19874fc61dbc48e6a02125a4a069af05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the gemma model from huggingface\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "\n",
    "#Downloading models locally to query them\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",  # Offloads intelligently between GPU & CPU\n",
    "    offload_folder=\"offload_dir\"  # Offload excess weights to disk (temporary)\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcd957b3-96d2-4952-8550-3a94b949fc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring getting response form llm\n",
    "def local_llm(prompt: str, processor, model) -> str:\n",
    "    \"\"\"\n",
    "    Sends a prompt to a HuggingFace Gemma model and returns the response, using GPU if available.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Step 1: Tokenize the messages\n",
    "        inputs = processor.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Step 2: Move inputs to the model's device (GPU or CPU)\n",
    "        device = model.device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Step 3: Generate output with inference mode\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1024,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "\n",
    "        # Step 4: Decode generated output\n",
    "        decoded_output = processor.batch_decode(\n",
    "            outputs,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )[0]\n",
    "\n",
    "        return decoded_output.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in HuggingFace LLM call: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fa5af53-0931-40bc-8e47-744e3cc8f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk_to_alpaca(doc: Document, processor, model) -> dict:\n",
    "    source_name = doc.metadata.get(\"source\", \"Unknown Name\")\n",
    "\n",
    "    instruction_with_metadata = f\"\"\"\n",
    "You are a business assistant analyzing raw business content from the following source:\n",
    "SOURCE NAME: {source_name}\n",
    "\n",
    "Your task is to extract the following from the provided transcript:\n",
    "1. Frameworks (e.g., naming, advertising, validation models).\n",
    "2. Bullet points for key ideas or steps.\n",
    "3. Q&A (any implied or stated questions with answers).\n",
    "4. Case Examples or stories.\n",
    "5. Copywriting formulas (AIDA, PAS, etc.)\n",
    "6. Classify this content into high-level topics: e.g., Naming, Ads, Psychology, Copywriting.\n",
    "7. Convert suitable content into a step-by-step guide.\n",
    "\n",
    "Return your output in clearly labeled sections, and only include sections with relevant content. Do not include a preamble.\n",
    "\"\"\".strip()\n",
    "\n",
    "    prompt = f\"{instruction_with_metadata}\\n\\n{doc.page_content.strip()}\"\n",
    "    response = local_llm(prompt, processor, model)\n",
    "\n",
    "    return {\n",
    "        \"instruction\": instruction_with_metadata,\n",
    "        \"input\": doc.page_content.strip(),\n",
    "        \"output\": response,\n",
    "        \"metadata\": doc.metadata\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502877fa-49da-4078-9bf4-60c009c36941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:  50%|█████     | 2/4 [01:10<01:12, 36.02s/it]"
     ]
    }
   ],
   "source": [
    "# Test Time Taken for first 3 docs\n",
    "alpaca_data = []\n",
    "\n",
    "for doc in tqdm(all_marketing_pages[27:31], desc=\"Processing documents\", leave=True):\n",
    "    alpaca_entry = process_chunk_to_alpaca(doc, processor, model)\n",
    "    alpaca_data.append(alpaca_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940bb303-d4a0-4bbd-b21f-c96c3a02f8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process All\n",
    "alpaca_data = []\n",
    "\n",
    "for doc in tqdm(all_marketing_pages, desc=\"Processing documents\", leave=True):\n",
    "    alpaca_entry = process_chunk_to_alpaca(doc, processor, model)\n",
    "    alpaca_data.append(alpaca_entry)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
