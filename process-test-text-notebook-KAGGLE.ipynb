{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12003129,"sourceType":"datasetVersion","datasetId":7550746}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install -U transformers","metadata":{"colab_type":"code","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#For offloading models to GPU.\n#!pip install -q accelerate transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Importing all required packages\nimport pickle\nimport json\nfrom tqdm import tqdm\nimport time\nimport os\nfrom huggingface_hub import login\nfrom langchain.schema import Document\nimport torch\n\n# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\n#Cuda memory will faill without this\n# import os\n# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n#Importing secretkey saved in KAggle secrets for logging into huggingface\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n\n#Logging into huggingface liek this as CLI is not working for us in Kaggle\nlogin(token=secret_value_0)\n\n#Importing packages required to download model\nimport requests\nfrom transformers import AutoProcessor, Gemma3ForConditionalGeneration\n\n#For offloading model to GPU\nfrom accelerate import infer_auto_device_map, init_empty_weights\nfrom transformers import AutoConfig","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-30T08:18:52.316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #Cuda will report exact line of error\n# import os\n# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n# os.environ[\"PYTORCH_USE_CUDA_DSA\"] = \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:42:41.722843Z","iopub.execute_input":"2025-05-30T06:42:41.723521Z","iopub.status.idle":"2025-05-30T06:42:41.727037Z","shell.execute_reply.started":"2025-05-30T06:42:41.723497Z","shell.execute_reply":"2025-05-30T06:42:41.726283Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"with open(\"/kaggle/input/all-marketing-material/all_marketing_material.pkl\", \"rb\") as f:\n    all_marketing_pages = pickle.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:58:53.864102Z","iopub.execute_input":"2025-05-30T06:58:53.865054Z","iopub.status.idle":"2025-05-30T06:58:54.028261Z","shell.execute_reply.started":"2025-05-30T06:58:53.865030Z","shell.execute_reply":"2025-05-30T06:58:54.027437Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"len(all_marketing_pages)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:58:55.076985Z","iopub.execute_input":"2025-05-30T06:58:55.077711Z","iopub.status.idle":"2025-05-30T06:58:55.082522Z","shell.execute_reply.started":"2025-05-30T06:58:55.077688Z","shell.execute_reply":"2025-05-30T06:58:55.081942Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"6608"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"model_id = \"google/gemma-3-4b-it\"\n\n#Downloading models locally to query them\nmodel = Gemma3ForConditionalGeneration.from_pretrained(\n    model_id,\n    device_map=\"auto\",  # Offloads intelligently between GPU & CPU\n    offload_folder=\"offload_dir\"  # Offload excess weights to disk (temporary)\n)\nprocessor = AutoProcessor.from_pretrained(model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:59:07.080617Z","iopub.execute_input":"2025-05-30T06:59:07.081174Z","iopub.status.idle":"2025-05-30T06:59:55.102811Z","shell.execute_reply.started":"2025-05-30T06:59:07.081151Z","shell.execute_reply":"2025-05-30T06:59:55.102243Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51ae8748fbaf4ddd9dd826228b23ae7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b19b48a162294a99bdf9f0a4c44453ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb0c0d55bc3f45c5b6212d819f95477a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab9e4eec6e6f4df99b20543b5efdd94d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c2f2a94335c4354b5bc380acb84f161"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8908492fa57441f78651f82a02c657b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fda85fb8d1748969ed9742fb5c05a43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a45ae782d634a72893acde0bd332b8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6471019c2b64f6b9a74741aea3d726b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a65c41296a24db9a685d1ad8e751584"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cd8b13e42b9498a9c8ae3f04e7ace18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1075dbb4f35549b8be27aab171376839"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd4ad1ce0d864bb88b67a753773474ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a89aa1994e5b42b1b55eb7ba89124bd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a515bf24ebab445e9045b113471bd470"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# #Downloading models locally to query them\n# model = Gemma3ForConditionalGeneration.from_pretrained(\"google/gemma-3-4b-it\")\n# processor = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Testing, its working when Models are in GPU\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}\n        ]\n    },\n    {\n        \"role\": \"user\", \"content\": [\n            {\"type\": \"text\", \"text\": \"What is machine learning?\"},\n        ]\n    },\n]\n\ninputs = processor.apply_chat_template(\n    messages,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n    add_generation_prompt=True\n)\n# Generate\ngenerate_ids = model.generate(**inputs)\nprocessor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T06:54:53.969468Z","iopub.execute_input":"2025-05-30T06:54:53.969765Z","iopub.status.idle":"2025-05-30T06:54:56.865658Z","shell.execute_reply.started":"2025-05-30T06:54:53.969744Z","shell.execute_reply":"2025-05-30T06:54:56.865027Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2347: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"\"user\\nYou are a helpful assistant.\\n\\nWhat is machine learning?\\nmodel\\nOkay, let's break down what machine learning is! Here's a breakdown in a way\""},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Models on GPU but runs queries on CPU!\n# def local_llm(prompt: str, processor, model) -> str:\n#     \"\"\"\n#     Sends a prompt to a HuggingFace Gemma model and returns the response.\n#     \"\"\"\n#     messages = [\n#         {\n#             \"role\": \"system\",\n#             \"content\": [\n#                 {\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}\n#             ]\n#         },\n#         {\n#             \"role\": \"user\",\n#             \"content\": [\n#                 {\"type\": \"text\", \"text\": prompt}\n#             ]\n#         }\n#     ]\n\n#     try:\n#         inputs = processor.apply_chat_template(\n#             messages,\n#             tokenize=True,\n#             return_dict=True,\n#             return_tensors=\"pt\",\n#             add_generation_prompt=True\n#         ).to(model.device)\n\n#         with torch.inference_mode():\n#             outputs = model.generate(**inputs, max_new_tokens=1024)\n        \n#         # Decode only the new tokens (not input prompt)\n#         decoded_output = processor.batch_decode(\n#             outputs, skip_special_tokens=True, clean_up_tokenization_spaces=True\n#         )[0]\n\n#         return decoded_output.strip()\n    \n#     except Exception as e:\n#         print(f\"Error in HuggingFace LLM call: {e}\")\n#         return \"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:00:14.750182Z","iopub.execute_input":"2025-05-30T07:00:14.750463Z","iopub.status.idle":"2025-05-30T07:00:14.756047Z","shell.execute_reply.started":"2025-05-30T07:00:14.750434Z","shell.execute_reply":"2025-05-30T07:00:14.755435Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# # This is one more working piece fo code, thats running on GPU\n# messages = [\n#     {\n#         \"role\": \"system\",\n#         \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n#     },\n#     {\n#         \"role\": \"user\",\n#         \"content\": [{\"type\": \"text\", \"text\": \"Explain what marketing funnels are.\"}]\n#     }\n# ]\n\n# # Step 3: Prepare inputs (tokenized + moved to model's device)\n# inputs = processor.apply_chat_template(\n#     messages,\n#     tokenize=True,\n#     return_dict=True,\n#     return_tensors=\"pt\",\n#     add_generation_prompt=True\n# )\n\n# # 🧠 Step 4: Move inputs to the **same device** as model\n# device = model.device  # Auto-detected GPU device\n# inputs = {k: v.to(device) for k, v in inputs.items()}\n\n# # Step 5: Generate response\n# with torch.inference_mode():\n#     outputs = model.generate(\n#         **inputs,\n#         max_new_tokens=512\n#     )\n\n# # Step 6: Decode response\n# decoded_output = processor.batch_decode(\n#     outputs,\n#     skip_special_tokens=True,\n#     clean_up_tokenization_spaces=True\n# )[0]\n\n# print(decoded_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:08:02.682852Z","iopub.execute_input":"2025-05-30T07:08:02.683155Z","iopub.status.idle":"2025-05-30T07:08:51.961769Z","shell.execute_reply.started":"2025-05-30T07:08:02.683134Z","shell.execute_reply":"2025-05-30T07:08:51.961132Z"}},"outputs":[{"name":"stdout","text":"user\nYou are a helpful assistant.\n\nExplain what marketing funnels are.\nmodel\nOkay, let's break down marketing funnels! They're a really valuable tool for understanding and improving how you attract and convert customers. Here’s a clear explanation:\n\n**What is a Marketing Funnel?**\n\nA marketing funnel is a visual representation of the customer journey – the steps a potential customer takes from first becoming aware of your brand to ultimately becoming a paying customer. It’s called a “funnel” because it naturally narrows down as people move through the stages.  Lots of people enter at the top, but fewer and fewer make it all the way to the bottom.\n\n**The Classic Stages of a Marketing Funnel:**\n\nWhile variations exist, the most common model includes these stages:\n\n1. **Awareness (Top of the Funnel - TOFU):**\n   * **What it is:** This is where potential customers first *hear* about you. They might see your social media post, read a blog article, hear an ad, or stumble upon your website.\n   * **Goal:** Get your brand in front of as many relevant people as possible.\n   * **Metrics:** Impressions, reach, website traffic.\n\n\n2. **Interest (Middle of the Funnel - MOFU):**\n   * **What it is:**  Now that people know about you, they’re starting to show *interest*. They might visit your website, download a lead magnet (like an ebook or checklist), subscribe to your newsletter, or engage with your social media content.\n   * **Goal:** Capture leads (potential customers) and provide them with valuable information that addresses their needs.\n   * **Metrics:**  Lead generation, email open rates, click-through rates, time on site.\n\n\n3. **Consideration (Middle of the Funnel - MOFU):**\n   * **What it is:**  Potential customers are actively *evaluating* your product or service. They’re comparing you to competitors, reading reviews, and maybe even requesting a demo.\n   * **Goal:**  Showcase the value of your offering and why it’s the best solution for their problem.\n   * **Metrics:**  Demo requests, webinar registrations, product page views, comparison shopping.\n\n\n4. **Decision (Bottom of the Funnel - BOFU):**\n   * **What it is:** The customer is ready to *buy*! They’re likely looking for pricing information, addressing final concerns, and possibly talking to a salesperson.\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Both on cuda 0 means both are on GPU\nprint(\"Model is on:\", model.device)\nprint(\"Inputs are on:\", inputs[\"input_ids\"].device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:08:51.962716Z","iopub.execute_input":"2025-05-30T07:08:51.963009Z","iopub.status.idle":"2025-05-30T07:08:51.967352Z","shell.execute_reply.started":"2025-05-30T07:08:51.962988Z","shell.execute_reply":"2025-05-30T07:08:51.966432Z"}},"outputs":[{"name":"stdout","text":"Model is on: cuda:0\nInputs are on: cuda:0\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"#Lets test if this works on GPU\ndef local_llm(prompt: str, processor, model) -> str:\n    \"\"\"\n    Sends a prompt to a HuggingFace Gemma model and returns the response, using GPU if available.\n    \"\"\"\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [{\"type\": \"text\", \"text\": prompt}]\n        }\n    ]\n\n    try:\n        # Step 1: Tokenize the messages\n        inputs = processor.apply_chat_template(\n            messages,\n            tokenize=True,\n            return_dict=True,\n            return_tensors=\"pt\",\n            add_generation_prompt=True\n        )\n\n        # Step 2: Move inputs to the model's device (GPU or CPU)\n        device = model.device\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n\n        # Step 3: Generate output with inference mode\n        with torch.inference_mode():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=1024,\n                do_sample=True,\n                temperature=0.7\n            )\n\n        # Step 4: Decode generated output\n        decoded_output = processor.batch_decode(\n            outputs,\n            skip_special_tokens=True,\n            clean_up_tokenization_spaces=True\n        )[0]\n\n        return decoded_output.strip()\n\n    except Exception as e:\n        print(f\"Error in HuggingFace LLM call: {e}\")\n        return \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:11:37.820856Z","iopub.execute_input":"2025-05-30T07:11:37.821377Z","iopub.status.idle":"2025-05-30T07:11:37.827343Z","shell.execute_reply.started":"2025-05-30T07:11:37.821356Z","shell.execute_reply":"2025-05-30T07:11:37.826663Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def process_chunk_to_alpaca(doc: Document, processor, model) -> dict:\n    source_name = doc.metadata.get(\"source\", \"Unknown Name\")\n\n    instruction_with_metadata = f\"\"\"\nYou are a business assistant analyzing raw business content from the following source:\nSOURCE NAME: {source_name}\n\nYour task is to extract the following from the provided transcript:\n1. Frameworks (e.g., naming, advertising, validation models).\n2. Bullet points for key ideas or steps.\n3. Q&A (any implied or stated questions with answers).\n4. Case Examples or stories.\n5. Copywriting formulas (AIDA, PAS, etc.)\n6. Classify this content into high-level topics: e.g., Naming, Ads, Psychology, Copywriting.\n7. Convert suitable content into a step-by-step guide.\n\nReturn your output in clearly labeled sections, and only include sections with relevant content. Do not include a preamble.\n\"\"\".strip()\n\n    prompt = f\"{instruction_with_metadata}\\n\\n{doc.page_content.strip()}\"\n    response = local_llm(prompt, processor, model)\n\n    return {\n        \"instruction\": instruction_with_metadata,\n        \"input\": doc.page_content.strip(),\n        \"output\": response,\n        \"metadata\": doc.metadata\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:11:43.182772Z","iopub.execute_input":"2025-05-30T07:11:43.183047Z","iopub.status.idle":"2025-05-30T07:11:43.187860Z","shell.execute_reply.started":"2025-05-30T07:11:43.183026Z","shell.execute_reply":"2025-05-30T07:11:43.187075Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"alpaca_data = []\n\nfor doc in all_marketing_pages[:1]:  # or full range\n    alpaca_entry = process_chunk_to_alpaca(doc, processor, model)\n    alpaca_data.append(alpaca_entry)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:11:51.614095Z","iopub.execute_input":"2025-05-30T07:11:51.614642Z","iopub.status.idle":"2025-05-30T07:13:07.923694Z","shell.execute_reply.started":"2025-05-30T07:11:51.614623Z","shell.execute_reply":"2025-05-30T07:13:07.923167Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"alpaca_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:14:10.582156Z","iopub.execute_input":"2025-05-30T07:14:10.582427Z","iopub.status.idle":"2025-05-30T07:14:10.588581Z","shell.execute_reply.started":"2025-05-30T07:14:10.582408Z","shell.execute_reply":"2025-05-30T07:14:10.587802Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[{'instruction': 'You are a business assistant analyzing raw business content from the following source:\\nSOURCE NAME: Alex Hormozi 100 million leads\\n\\nYour task is to extract the following from the provided transcript:\\n1. Frameworks (e.g., naming, advertising, validation models).\\n2. Bullet points for key ideas or steps.\\n3. Q&A (any implied or stated questions with answers).\\n4. Case Examples or stories.\\n5. Copywriting formulas (AIDA, PAS, etc.)\\n6. Classify this content into high-level topics: e.g., Naming, Ads, Psychology, Copywriting.\\n7. Convert suitable content into a step-by-step guide.\\n\\nReturn your output in clearly labeled sections, and only include sections with relevant content. Do not include a preamble.',\n  'input': 'A c q u i s i t i o n . c o m  V o l u m e  I I\\n \\n$100M Leads\\n \\nH o w  t o  G e t  S t r a n g e r s  T o\\nW a n t  T o  B u y  Y o u r  S t u f f\\n \\n \\nA l e x  H o r m o z i\\nCopyright © 2023 by Alex Hormozi\\n \\nAll rights reserved. No part of this publication may be reproduced,\\ndistributed, or transmitted in any form or by any means, including\\nphotocopying, recording, or other electronic or mechanical\\nmethods, without the prior written permission of the publisher ,\\nexcept in the case of brief questions embodied in critical reviews\\nand certain other noncommercial uses permitted by copyright law .\\nFor permission requests, write to the publisher at the address\\nbelow .\\nAcquisition.com\\n7710 N FM 620, Building 13C, Suite 100,\\nAustin, T exas 78726\\nGuiding Principles\\n \\n \\n \\nD o  m o r e .\\n \\n \\nT h a n k  Y o u s\\n \\n \\nT o T r evor:\\nThank you for your true friendship. Thank you for your tireless\\nef fort to extract the ideas out of my head. And, for your continued\\nsupport in slaying the nihilism monster . People say you are lucky if\\nyou have one real friend in your entire life. Thank you for being\\nthe best friend a man could ask for .\\n \\nT o Leila:\\nEven though Lady Gaga said it first, it doesn’ t make it any less\\ntrue.\\n“Y ou found the light in me that I couldn’ t find.\\nThe part of me that’ s you will never die.”\\n \\nT able of Contents\\nSection I: Start Her e\\nHow I Got Here\\nThe Problem This Book Solves\\nSection II: Get Understanding\\nLeads Alone Aren’ t Enough\\nEngage Y our Leads: Of fers and Lead Magnets\\nSection III: Get Leads\\n#1 W arm Outreach\\n#2 Post Free Content Part I\\n#2 Post Free Content Part II\\nFree Goodwill\\n#3 Cold Outreach\\n#4 Run Paid Ads Part I: Making An Ad\\n#4 Run Paid Ads Part II: Money Stuf f\\nCore Four On Steroids: More Better New\\nSection IV : Get Lead Getters\\n#1 Customer Referrals - W ord of Mouth\\n#2 Employees\\n#3 Agencies\\n#4 Af filiates and Partners\\nSection IV Conclusion: Get Lead Getters\\nSection V : Get Started\\nAdvertising in Real Life: Open T o Goal',\n  'output': 'user\\nYou are a helpful assistant.\\n\\nYou are a business assistant analyzing raw business content from the following source:\\nSOURCE NAME: Alex Hormozi 100 million leads\\n\\nYour task is to extract the following from the provided transcript:\\n1. Frameworks (e.g., naming, advertising, validation models).\\n2. Bullet points for key ideas or steps.\\n3. Q&A (any implied or stated questions with answers).\\n4. Case Examples or stories.\\n5. Copywriting formulas (AIDA, PAS, etc.)\\n6. Classify this content into high-level topics: e.g., Naming, Ads, Psychology, Copywriting.\\n7. Convert suitable content into a step-by-step guide.\\n\\nReturn your output in clearly labeled sections, and only include sections with relevant content. Do not include a preamble.\\n\\nA c q u i s i t i o n. c o m  V o l u m e  I I\\n \\n$100M Leads\\n \\nH o w  t o  G e t  S t r a n g e r s  T o\\nW a n t  T o  B u y  Y o u r  S t u f f\\n \\n \\nA l e x  H o r m o z i\\nCopyright © 2023 by Alex Hormozi\\n \\nAll rights reserved. No part of this publication may be reproduced,\\ndistributed, or transmitted in any form or by any means, including\\nphotocopying, recording, or other electronic or mechanical\\nmethods, without the prior written permission of the publisher,\\nexcept in the case of brief questions embodied in critical reviews\\nand certain other noncommercial uses permitted by copyright law.\\nFor permission requests, write to the publisher at the address\\nbelow.\\nAcquisition.com\\n7710 N FM 620, Building 13C, Suite 100,\\nAustin, T exas 78726\\nGuiding Principles\\n \\n \\n \\nD o  m o r e.\\n \\n \\nT h a n k  Y o u s\\n \\n \\nT o T r evor:\\nThank you for your true friendship. Thank you for your tireless\\nef fort to extract the ideas out of my head. And, for your continued\\nsupport in slaying the nihilism monster. People say you are lucky if\\nyou have one real friend in your entire life. Thank you for being\\nthe best friend a man could ask for.\\n \\nT o Leila:\\nEven though Lady Gaga said it first, it doesn’ t make it any less\\ntrue.\\n“Y ou found the light in me that I couldn’ t find.\\nThe part of me that’ s you will never die.”\\n \\nT able of Contents\\nSection I: Start Her e\\nHow I Got Here\\nThe Problem This Book Solves\\nSection II: Get Understanding\\nLeads Alone Aren’ t Enough\\nEngage Y our Leads: Of fers and Lead Magnets\\nSection III: Get Leads\\n#1 W arm Outreach\\n#2 Post Free Content Part I\\n#2 Post Free Content Part II\\nFree Goodwill\\n#3 Cold Outreach\\n#4 Run Paid Ads Part I: Making An Ad\\n#4 Run Paid Ads Part II: Money Stuf f\\nCore Four On Steroids: More Better New\\nSection IV : Get Lead Getters\\n#1 Customer Referrals - W ord of Mouth\\n#2 Employees\\n#3 Agencies\\n#4 Af filiates and Partners\\nSection IV Conclusion: Get Lead Getters\\nSection V : Get Started\\nAdvertising in Real Life: Open T o Goal\\nmodel\\n## Frameworks\\n\\n*   **Core Four On Steroids:** This is a framework for scaling lead generation, building upon the foundational four strategies (Customer Referrals, Employees, Agencies, Affiliates) and amplifying their effectiveness.\\n*   **Leads Alone Aren’t Enough:** This highlights the need to go beyond simply acquiring leads and focus on engagement.\\n\\n## Bullet Points for Key Ideas or Steps\\n\\n*   **Do More:**  A core principle emphasizing increased effort and action.\\n*   **Warm Outreach:** Prioritizing personalized, direct communication.\\n*   **Post Free Content:** Creating valuable content to attract potential customers.\\n*   **Cold Outreach:** Utilizing less personalized, broader outreach efforts.\\n*   **Run Paid Ads:** Implementing paid advertising campaigns.\\n*   **Customer Referrals - Word of Mouth:** Leveraging existing customers for new leads.\\n*   **Employees:** Utilizing employees as a lead generation channel.\\n*   **Agencies & Affiliates:** Partnering with external organizations for lead acquisition.\\n\\n\\n## Q&A\\n\\n*   **Question:** What is the fundamental principle presented?\\n    *   **Answer:** “Do More.”\\n*   **Question:** What’s more important than just acquiring leads?\\n    *   **Answer:** Engaging leads with offers and lead magnets.\\n\\n## Case Examples or Stories\\n\\n*   **Anecdote about Trevor & Leila:**  A personal story acknowledging the support of friends (Trevor and Leila), expressing gratitude for their contributions. (This isn\\'t a business case, but illustrates the author\\'s values).\\n\\n## Copywriting Formulas\\n\\n*   The text doesn’t explicitly detail a specific copywriting formula like AIDA or PAS. However, the emphasis on offers and lead magnets suggests a focus on providing value to attract attention and generate interest.\\n\\n## Classify this Content\\n\\n*   **Lead Generation:**  The primary focus is on strategies for acquiring leads.\\n*   **Marketing Strategy:**  The content outlines a multi-faceted approach to lead generation.\\n*   **Business Growth:** The framework is presented as a means to scale business operations.\\n\\n## Step-by-Step Guide (Based on Sections)\\n\\n**Step-by-Step Guide to Generating Leads (Based on the Provided Content)**\\n\\n**Phase 1: Foundation - Getting Started**\\n\\n1.  **Embrace \"Do More\":** Commit to increased effort and action in your lead generation efforts.\\n2.  **Understand the Problem:** Recognize that simply having leads isn\\'t enough; engagement is crucial.\\n\\n**Phase 2: Lead Acquisition – Core Strategies**\\n\\n3.  **Warm Outreach:** Start with personalized, direct communication with potential customers.\\n4.  **Post Free Content (Part 1 & 2):** Create valuable content (blog posts, videos, guides) to attract a wider audience.\\n5.  **Cold Outreach:** Supplement warm outreach with broader, less personalized efforts.\\n6.  **Run Paid Ads (Part 1 - Making an Ad):** Develop compelling advertisements to reach a targeted audience.\\n7.  **Run Paid Ads (Part 2 - Money Stuff):** Allocate budget and manage paid advertising campaigns effectively.\\n\\n**Phase 3: Scaling Lead Generation - Leverage Channels**\\n\\n8.  **Customer Referrals (Word of Mouth):** Encourage existing customers to refer new leads.\\n9.  **Employees:** Utilize employees as a source of leads through referrals or internal marketing.\\n10. **Agencies & Affiliates:** Partner with external organizations (agencies, affiliates) to expand your reach.\\n\\n**Phase 4: Continuous Improvement**\\n\\n11. **Review & Refine:** Regularly assess the effectiveness of each lead generation strategy and adjust your approach accordingly.',\n  'metadata': {'source': 'Alex Hormozi 100 million leads'}}]"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Lets make a state managed code that processes all docs with tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PROCESSED_FILE = \"alpaca_processed.jsonl\"\nFAILED_FILE = \"alpaca_failed.jsonl\"\nMAX_RETRIES = 3\nRETRY_DELAY = 2  # seconds between retries\n\ndef load_jsonl_ids(filename):\n    if not os.path.exists(filename):\n        return set()\n    with open(filename, \"r\") as f:\n        return {json.loads(line).get(\"metadata\", {}).get(\"source\", \"\") + str(json.loads(line).get(\"metadata\", {}).get(\"page\", \"\")) for line in f}\n\ndef save_jsonl(filename, data):\n    with open(filename, \"a\") as f:\n        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n\ndef get_doc_id(doc: Document):\n    source = doc.metadata.get(\"source\", \"\")\n    page = doc.metadata.get(\"page\")\n\n    if page is None:\n        # Fallback to hashing part of the content if page is missing\n        content_hash = str(abs(hash(doc.page_content[:50])))\n        return f\"{source}_hash_{content_hash}\"\n\n    return f\"{source}_page_{page}\"\n\ndef process_documents_with_retries(pages, processor, model):\n    processed_ids = load_jsonl_ids(PROCESSED_FILE)\n    failed_ids = load_jsonl_ids(FAILED_FILE)\n\n    for doc in tqdm(pages, desc=\"Processing documents\"):\n        doc_id = get_doc_id(doc)\n\n        if doc_id in processed_ids:\n            continue\n\n        retries = 0\n        success = False\n\n        while retries < MAX_RETRIES and not success:\n            try:\n                alpaca_entry = process_chunk_to_alpaca(doc, processor, model)\n                save_jsonl(PROCESSED_FILE, alpaca_entry)\n                success = True\n            except Exception as e:\n                retries += 1\n                if retries < MAX_RETRIES:\n                    time.sleep(RETRY_DELAY)\n                else:\n                    error_entry = {\n                        \"error\": str(e),\n                        \"metadata\": doc.metadata,\n                        \"input\": doc.page_content[:500]  # preview of failed input\n                    }\n                    save_jsonl(FAILED_FILE, error_entry)\n        time.sleep(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:32:37.469314Z","iopub.execute_input":"2025-05-30T07:32:37.469836Z","iopub.status.idle":"2025-05-30T07:32:37.477768Z","shell.execute_reply.started":"2025-05-30T07:32:37.469816Z","shell.execute_reply":"2025-05-30T07:32:37.477123Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"process_documents_with_retries(all_marketing_pages, processor, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:32:41.978233Z","iopub.execute_input":"2025-05-30T07:32:41.978500Z"}},"outputs":[{"name":"stderr","text":"Processing documents:   0%|          | 7/6608 [07:03<108:25:02, 59.13s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}