{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-30T08:18:52.316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Importing all required packages\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from langchain.schema import Document\n",
    "import torch\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "#Cuda memory will faill without this\n",
    "# import os\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "#Importing secretkey saved in KAggle secrets for logging into huggingface\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "#Logging into huggingface liek this as CLI is not working for us in Kaggle\n",
    "login(token=secret_value_0)\n",
    "\n",
    "#Importing packages required to download model\n",
    "import requests\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "\n",
    "#For offloading model to GPU\n",
    "from accelerate import infer_auto_device_map, init_empty_weights\n",
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T06:58:53.865054Z",
     "iopub.status.busy": "2025-05-30T06:58:53.864102Z",
     "iopub.status.idle": "2025-05-30T06:58:54.028261Z",
     "shell.execute_reply": "2025-05-30T06:58:54.027437Z",
     "shell.execute_reply.started": "2025-05-30T06:58:53.865030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/all-marketing-material/all_marketing_material.pkl\", \"rb\") as f:\n",
    "    all_marketing_pages = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T06:59:07.081174Z",
     "iopub.status.busy": "2025-05-30T06:59:07.080617Z",
     "iopub.status.idle": "2025-05-30T06:59:55.102811Z",
     "shell.execute_reply": "2025-05-30T06:59:55.102243Z",
     "shell.execute_reply.started": "2025-05-30T06:59:07.081151Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ae8748fbaf4ddd9dd826228b23ae7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19b48a162294a99bdf9f0a4c44453ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0c0d55bc3f45c5b6212d819f95477a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9e4eec6e6f4df99b20543b5efdd94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2f2a94335c4354b5bc380acb84f161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8908492fa57441f78651f82a02c657b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fda85fb8d1748969ed9742fb5c05a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a45ae782d634a72893acde0bd332b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6471019c2b64f6b9a74741aea3d726b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a65c41296a24db9a685d1ad8e751584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd8b13e42b9498a9c8ae3f04e7ace18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1075dbb4f35549b8be27aab171376839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4ad1ce0d864bb88b67a753773474ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89aa1994e5b42b1b55eb7ba89124bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a515bf24ebab445e9045b113471bd470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"google/gemma-3-4b-it\"\n",
    "\n",
    "#Downloading models locally to query them\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",  # Offloads intelligently between GPU & CPU\n",
    "    offload_folder=\"offload_dir\"  # Offload excess weights to disk (temporary)\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T07:11:37.821377Z",
     "iopub.status.busy": "2025-05-30T07:11:37.820856Z",
     "iopub.status.idle": "2025-05-30T07:11:37.827343Z",
     "shell.execute_reply": "2025-05-30T07:11:37.826663Z",
     "shell.execute_reply.started": "2025-05-30T07:11:37.821356Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Lets test if this works on GPU\n",
    "def local_llm(prompt: str, processor, model) -> str:\n",
    "    \"\"\"\n",
    "    Sends a prompt to a HuggingFace Gemma model and returns the response, using GPU if available.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Step 1: Tokenize the messages\n",
    "        inputs = processor.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Step 2: Move inputs to the model's device (GPU or CPU)\n",
    "        device = model.device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Step 3: Generate output with inference mode\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1024,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "\n",
    "        # Step 4: Decode generated output\n",
    "        decoded_output = processor.batch_decode(\n",
    "            outputs,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )[0]\n",
    "\n",
    "        return decoded_output.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in HuggingFace LLM call: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T07:11:43.183047Z",
     "iopub.status.busy": "2025-05-30T07:11:43.182772Z",
     "iopub.status.idle": "2025-05-30T07:11:43.187860Z",
     "shell.execute_reply": "2025-05-30T07:11:43.187075Z",
     "shell.execute_reply.started": "2025-05-30T07:11:43.183026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_chunk_to_alpaca(doc: Document, processor, model) -> dict:\n",
    "    source_name = doc.metadata.get(\"source\", \"Unknown Name\")\n",
    "\n",
    "    instruction_with_metadata = f\"\"\"\n",
    "You are a business assistant analyzing raw business content from the following source:\n",
    "SOURCE NAME: {source_name}\n",
    "\n",
    "Your task is to extract the following from the provided transcript:\n",
    "1. Frameworks (e.g., naming, advertising, validation models).\n",
    "2. Bullet points for key ideas or steps.\n",
    "3. Q&A (any implied or stated questions with answers).\n",
    "4. Case Examples or stories.\n",
    "5. Copywriting formulas (AIDA, PAS, etc.)\n",
    "6. Classify this content into high-level topics: e.g., Naming, Ads, Psychology, Copywriting.\n",
    "7. Convert suitable content into a step-by-step guide.\n",
    "\n",
    "Return your output in clearly labeled sections, and only include sections with relevant content. Do not include a preamble.\n",
    "\"\"\".strip()\n",
    "\n",
    "    prompt = f\"{instruction_with_metadata}\\n\\n{doc.page_content.strip()}\"\n",
    "    response = local_llm(prompt, processor, model)\n",
    "\n",
    "    return {\n",
    "        \"instruction\": instruction_with_metadata,\n",
    "        \"input\": doc.page_content.strip(),\n",
    "        \"output\": response,\n",
    "        \"metadata\": doc.metadata\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Lets make a state managed code that processes all docs with tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T07:32:37.469836Z",
     "iopub.status.busy": "2025-05-30T07:32:37.469314Z",
     "iopub.status.idle": "2025-05-30T07:32:37.477768Z",
     "shell.execute_reply": "2025-05-30T07:32:37.477123Z",
     "shell.execute_reply.started": "2025-05-30T07:32:37.469816Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PROCESSED_FILE = \"alpaca_processed.jsonl\"\n",
    "FAILED_FILE = \"alpaca_failed.jsonl\"\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 2  # seconds between retries\n",
    "\n",
    "def load_jsonl_ids(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        return set()\n",
    "    with open(filename, \"r\") as f:\n",
    "        return {json.loads(line).get(\"metadata\", {}).get(\"source\", \"\") + str(json.loads(line).get(\"metadata\", {}).get(\"page\", \"\")) for line in f}\n",
    "\n",
    "def save_jsonl(filename, data):\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def get_doc_id(doc: Document):\n",
    "    source = doc.metadata.get(\"source\", \"\")\n",
    "    page = doc.metadata.get(\"page\")\n",
    "\n",
    "    if page is None:\n",
    "        # Fallback to hashing part of the content if page is missing\n",
    "        content_hash = str(abs(hash(doc.page_content[:50])))\n",
    "        return f\"{source}_hash_{content_hash}\"\n",
    "\n",
    "    return f\"{source}_page_{page}\"\n",
    "\n",
    "def process_documents_with_retries(pages, processor, model):\n",
    "    processed_ids = load_jsonl_ids(PROCESSED_FILE)\n",
    "    failed_ids = load_jsonl_ids(FAILED_FILE)\n",
    "\n",
    "    for doc in tqdm(pages, desc=\"Processing documents\"):\n",
    "        doc_id = get_doc_id(doc)\n",
    "\n",
    "        if doc_id in processed_ids:\n",
    "            continue\n",
    "\n",
    "        retries = 0\n",
    "        success = False\n",
    "\n",
    "        while retries < MAX_RETRIES and not success:\n",
    "            try:\n",
    "                alpaca_entry = process_chunk_to_alpaca(doc, processor, model)\n",
    "                save_jsonl(PROCESSED_FILE, alpaca_entry)\n",
    "                success = True\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                if retries < MAX_RETRIES:\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                else:\n",
    "                    error_entry = {\n",
    "                        \"error\": str(e),\n",
    "                        \"metadata\": doc.metadata,\n",
    "                        \"input\": doc.page_content[:500]  # preview of failed input\n",
    "                    }\n",
    "                    save_jsonl(FAILED_FILE, error_entry)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T07:32:41.978500Z",
     "iopub.status.busy": "2025-05-30T07:32:41.978233Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   0%|          | 7/6608 [07:03<108:25:02, 59.13s/it]"
     ]
    }
   ],
   "source": [
    "process_documents_with_retries(all_marketing_pages, processor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7550746,
     "sourceId": 12003129,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
