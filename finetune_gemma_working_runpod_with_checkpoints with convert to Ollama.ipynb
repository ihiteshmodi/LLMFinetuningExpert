{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f466d89-a76b-4d9a-91f9-1eef52261ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be used with runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bcd87c3-9d82-4a07-880d-6bfec6645a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each chekcpoint in this notebook is 130 mb, old are not deleted when new are here, calculate required storage as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9960d1bf-e47f-4a31-ae89-263c396362d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e51bd55-0f97-4c29-8c62-5be93db793b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing stuff, onl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b8da1d-2476-42c7-a7e2-1176e79831cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall -y unsloth unsloth_zoo torch xformers transformers triton protobuf wheel peft trl accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b38e80c-5280-4590-8723-f9e3d6c4419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First: install correct torch\n",
    "!pip install torch==2.6.0 torchvision\n",
    "\n",
    "# Then: install remaining tools with pinned versions\n",
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "\n",
    "# Fix any known dependency mismatches\n",
    "!pip install \\\n",
    "    \"protobuf<4.0.0\" \\\n",
    "    \"wheel>=0.42.0\" \\\n",
    "    sentencepiece \\\n",
    "    \"datasets>=3.4.1\" \\\n",
    "    huggingface_hub \\\n",
    "    hf_transfer \\\n",
    "    transformers==4.51.3 \\\n",
    "    safetensors \\\n",
    "    msgspec \\\n",
    "    tyro \\\n",
    "    regex \\\n",
    "    rich\n",
    "\n",
    "# Finally: install Unsloth itself\n",
    "!pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "857b1cf7-bd83-4b5a-822d-a6d23a59b4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: We'll be using `/tmp/unsloth_compiled_cache` for temporary Unsloth patches.\n",
      "Standard import failed for UnslothCPOTrainer: No module named 'UnslothCPOTrainer'. Using tempfile instead!\n"
     ]
    }
   ],
   "source": [
    "#Loading Unsloth Models\n",
    "from unsloth import FastModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd2d4e7d-c5a0-4dfb-86a8-0d879970e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing other required models\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411dfe6-9314-48c6-b803-9205d5aa5579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging into hugigngface as some of our datasets are gated!\n",
    "from huggingface_hub import login\n",
    "readonly_token = \"<enterhere>\"\n",
    "read_and_write_token = \"<enterhere>\"\n",
    "login(token=read_and_write_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94841dff-f688-4aa8-9d8a-19cd045c040e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.1: Fast Gemma3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA A40. Num GPUs = 1. Max memory: 44.448 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c251ed578e4473c9b87900d74d6b7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee9a139243d4947a5d6fb0f9e41ba76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dedd41660fd4c66b3f98c63071ed821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a137cb7005f94f68b2cef3bc86ffa530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7eeecf709a4d08966b0657b64e4823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f8a90c2b2e4ef2b9284ab17d4e58f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dca311b45964be3a4c3fcbc08911497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfd7018d7f5447da13e1a65c492f824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267ee168ebd34299b2476174ceae88d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c304deea4b154953a44fff09d57a140a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b3c7469b9d44db803822ab82d8ee2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preparing our model\n",
    "max_seq_length_ = 4096\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "\n",
    "    # Other popular models!\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/Llama-3.3-70B\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
    "    \"unsloth/Phi-4\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it\",\n",
    "    max_seq_length = max_seq_length_, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6f56193-acc1-45e2-9ee5-e99afa1f9585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.language_model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "# Loading model base\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 8,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 8,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1264e139-7979-4a42-a731-3d5acd9e23b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188cc8c6dbd84e8b953d3877d6a2405d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b594be041bba44419f7b27dbaf31e6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "python-codes-25k.json:   0%|          | 0.00/26.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a4d63bbf214109ba7d2ee083d14c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "python-codes-25k.jsonl:   0%|          | 0.00/25.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4809bfb5a3040daa49c7cc1e850b49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/49626 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3593cf8006144eeb8654b6b782607905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b7c7e8195148e880d7d82d7f771a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)nthetic_text_to_sql_train.snappy.parquet:   0%|          | 0.00/32.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868cc07e5a4a4daca9f33b5c827451ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)ynthetic_text_to_sql_test.snappy.parquet:   0%|          | 0.00/1.90M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695505bfa9d94cd5aafe75fd3aefc5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb66eea679446dda81ea0d09059b885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d99ab59b854e798b8e55e0d4f20b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "clean_dataset.csv:   0%|          | 0.00/3.87M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e1ae2e40c648dd885c7a6e75c3e145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8886 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c214f5f914fc43f7945007a5d1592340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/587 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420eeb7e1a044512b42801014f0f9e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a957e9320047f697ef9e13e8115390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/15734 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aedc323e6f9b4c7f97ea7673a7bcbd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257452b265494544822f18c61319cb60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17d2535703f4a02a28bba7ed2e4f5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/709k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dcadcc4297549148e3f302d2c0dd90d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/469k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062adb08899b4fc2b556b70000513045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b092abfc00d148398d90234f30c925be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c6f629adc440bf91fdb9c20e7eb9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/149 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset Download from huggingface\n",
    "# 1. CodeAnalyst\n",
    "python_dataset = load_dataset(\"flytech/python-codes-25k\")\n",
    "# 2. Text-to-SQL\n",
    "sql_dataset = load_dataset(\"gretelai/synthetic_text_to_sql\")\n",
    "# 3. PySpark QA\n",
    "pyspark_dataset = load_dataset(\"irfanalisoomro/clean_generated_pyspark_answers\")\n",
    "# 4. Leetcode\n",
    "# leetcode_dataset = load_dataset(\"DenCT/leetcode-python-solutions-with-exaplanations\")\n",
    "leetcode_dataset = load_dataset(\"LimYeri/LeetCode_Python_Solutions_v2\")\n",
    "# 5. Neetcode\n",
    "neetcode_dataset = load_dataset(\"nischalon10/neetcode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a1e2360-31c6-493e-ae6e-5f9e2254089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample conversion function for each dataset type\n",
    "def convert_to_gemma_format(user_text, model_text):\n",
    "    return f\"<start_of_turn>user\\n{user_text}<end_of_turn>\\n<start_of_turn>model\\n{model_text}<end_of_turn>\"\n",
    "\n",
    "# --- Load your real datasets below instead of these small examples ---\n",
    "\n",
    "def format_python_dataset(example):\n",
    "    prompt = f\"{example['instruction']}\\n{example['input']}\".strip()\n",
    "    return {\"text\": convert_to_gemma_format(prompt, example[\"output\"])}\n",
    "\n",
    "# 2. Text-to-SQL dataset\n",
    "def format_sql_dataset(example):\n",
    "    prompt = f\"{example['sql_prompt']}\\n{example['sql_context']}\".strip()\n",
    "    model_reply = f\"{example['sql']}\\n\\nExplanation:\\n{example['sql_explanation']}\"\n",
    "    return {\"text\": convert_to_gemma_format(prompt, model_reply)}\n",
    "\n",
    "# 3. PySpark Q&A dataset\n",
    "def format_pyspark_dataset(example):\n",
    "    return {\"text\": convert_to_gemma_format(example[\"Question\"], example[\"Answer\"])}\n",
    "\n",
    "# 4. Leetcode solutions dataset\n",
    "# def format_leetcode_dataset(example):\n",
    "#     return {\"text\": convert_to_gemma_format(example[\"question_content\"], example[\"content\"])}\n",
    "\n",
    "def format_leetcode_dataset(example):\n",
    "    return {\"text\": convert_to_gemma_format(example[\"question_content\"], example[\"content\"])}\n",
    "\n",
    "def format_neetcode_dataset(example):\n",
    "    return {\"text\": convert_to_gemma_format(example[\"content\"], example[\"python\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26612c31-8eff-47cd-82d1-de524ca9b21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1f4eb4f18647b39753f2f1f6ee5c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49626 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73f778c22a54090b2de973c8077917e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b11e11001424265822ead2910686756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8886 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcfb524c5984fa0847d91c38754dd63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15734 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e83bcf34e3948eabda723204481e56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Map your formatting functions on each dataset's \"train\" split, removing original columns.\n",
    "formatted_python = python_dataset[\"train\"].map(\n",
    "    format_python_dataset, \n",
    "    remove_columns=python_dataset[\"train\"].column_names\n",
    ")\n",
    "formatted_sql = sql_dataset[\"train\"].map(\n",
    "    format_sql_dataset, \n",
    "    remove_columns=sql_dataset[\"train\"].column_names\n",
    ")\n",
    "formatted_pyspark = pyspark_dataset[\"train\"].map(\n",
    "    format_pyspark_dataset, \n",
    "    remove_columns=pyspark_dataset[\"train\"].column_names\n",
    ")\n",
    "formatted_leetcode = leetcode_dataset[\"train\"].map(\n",
    "    format_leetcode_dataset, \n",
    "    remove_columns=leetcode_dataset[\"train\"].column_names\n",
    ")\n",
    "formatted_neetcode = neetcode_dataset[\"train\"].map(\n",
    "    format_neetcode_dataset, \n",
    "    remove_columns=neetcode_dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcea9dd0-b1f6-4499-a881-d8f89247822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all formatted results into one Dataset containing only the \"text\" field.\n",
    "combined_texts = (\n",
    "    formatted_python[\"text\"] +\n",
    "    formatted_sql[\"text\"] +\n",
    "    formatted_pyspark[\"text\"] +\n",
    "    formatted_leetcode[\"text\"] +\n",
    "    formatted_neetcode[\"text\"]\n",
    ")\n",
    "gemma_dataset = Dataset.from_dict({\"text\": combined_texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fa36e87-b07b-4bd9-8c4f-ed18a95a1adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 176236\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb82ea24-4370-458a-8f6a-184e43e2eab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0ee28a56314986a56b95590877e728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/176236 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training the model with custom settings\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = gemma_dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 6,\n",
    "        gradient_accumulation_steps = 4,    # Use GA to mimic batch size!\n",
    "        output_dir = \"output_checkpoint\",   #Necessary, if you want to save checkpoint and later continue\n",
    "        save_strategy = \"steps\",            #Necessary, if you want to save checkpoint and later continue\n",
    "        save_steps = 25,                     #Necessary, if you want to save checkpoint and later continue\n",
    "        save_total_limit=3,                 #Only save latest 2 checkpoints!\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        # max_steps = 30,\n",
    "        learning_rate = 2e-5, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        dataset_num_proc=2,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4ea65d3-bdbc-4594-9aa9-6ea7267aeff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe5185643724efda4571b63a9f7de26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=96):   0%|          | 0/176236 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Next piece of training!\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04d10ba5-c7b0-48d8-8de0-56201c32737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id recommend using the refined dataset instead, config changes cause us issues later!\n",
    "\"\"\"\n",
    "def is_valid(example):\n",
    "    try:\n",
    "        _ = tokenizer(example[\"text\"])\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "filtered_dataset = dataset.filter(is_valid, num_proc=2)\n",
    "\"\"\"\n",
    "\n",
    "# ignore_index is a pyhton pandas command i believe, it causes issues in model and stops execution, so we have to add this piece or filte rout all training data with it\n",
    "if not hasattr(model.config, \"ignore_index\"):\n",
    "    model.config.ignore_index = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69be42e8-b7ac-4a8a-b312-d4bda386fdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\n",
      "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]                \n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1765 kB]\n",
      "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4476 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.8 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1246 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]\n",
      "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2984 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4630 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3295 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1553 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
      "Fetched 40.6 MB in 3s (15.0 MB/s)                             \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "Suggested packages:\n",
      "  zip\n",
      "The following NEW packages will be installed:\n",
      "  unzip\n",
      "0 upgraded, 1 newly installed, 0 to remove and 143 not upgraded.\n",
      "Need to get 175 kB of archives.\n",
      "After this operation, 386 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 unzip amd64 6.0-26ubuntu3.2 [175 kB]\n",
      "Fetched 175 kB in 0s (361 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package unzip.\n",
      "(Reading database ... 20729 files and directories currently installed.)\n",
      "Preparing to unpack .../unzip_6.0-26ubuntu3.2_amd64.deb ...\n",
      "Unpacking unzip (6.0-26ubuntu3.2) ...\n",
      "Setting up unzip (6.0-26ubuntu3.2) ...\n"
     ]
    }
   ],
   "source": [
    "# unzip is installed in colab, not  in runpod, so have to do if running on runpod!\n",
    "!apt-get update && apt-get install -y unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "244adf48-44f8-4465-94f3-6cbdbb8c801b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  output_checkpoint.zip\n",
      "  inflating: output_checkpoint/checkpoint-7343/trainer_state.json  \n",
      "  inflating: output_checkpoint/checkpoint-7343/rng_state.pth  \n",
      "  inflating: output_checkpoint/checkpoint-7343/scheduler.pt  \n",
      "  inflating: output_checkpoint/checkpoint-7343/optimizer.pt  \n",
      "  inflating: output_checkpoint/checkpoint-7343/training_args.bin  \n",
      "  inflating: output_checkpoint/checkpoint-7343/processor_config.json  \n",
      "  inflating: output_checkpoint/checkpoint-7343/chat_template.json  \n",
      "  inflating: output_checkpoint/checkpoint-7343/tokenizer.json  \n",
      "  inflating: output_checkpoint/checkpoint-7343/tokenizer.model  \n",
      "  inflating: output_checkpoint/checkpoint-7343/added_tokens.json  \n",
      "  inflating: output_checkpoint/checkpoint-7343/special_tokens_map.json  \n",
      "  inflating: output_checkpoint/checkpoint-7343/tokenizer_config.json  \n",
      "  inflating: output_checkpoint/checkpoint-7343/preprocessor_config.json  \n",
      "  inflating: output_checkpoint/checkpoint-7343/adapter_config.json  \n",
      "  inflating: output_checkpoint/checkpoint-7343/adapter_model.safetensors  \n",
      "  inflating: output_checkpoint/checkpoint-7343/README.md  \n",
      "  inflating: output_checkpoint/checkpoint-7325/trainer_state.json  \n",
      "  inflating: output_checkpoint/checkpoint-7325/rng_state.pth  \n",
      "  inflating: output_checkpoint/checkpoint-7325/scheduler.pt  \n",
      "  inflating: output_checkpoint/checkpoint-7325/optimizer.pt  \n",
      "  inflating: output_checkpoint/checkpoint-7325/training_args.bin  \n",
      "  inflating: output_checkpoint/checkpoint-7325/processor_config.json  \n",
      "  inflating: output_checkpoint/checkpoint-7325/chat_template.json  \n",
      "  inflating: output_checkpoint/checkpoint-7325/tokenizer.json  \n",
      "  inflating: output_checkpoint/checkpoint-7325/tokenizer.model  \n",
      "  inflating: output_checkpoint/checkpoint-7325/added_tokens.json  \n",
      "  inflating: output_checkpoint/checkpoint-7325/special_tokens_map.json  \n",
      "  inflating: output_checkpoint/checkpoint-7325/tokenizer_config.json  \n",
      "  inflating: output_checkpoint/checkpoint-7325/preprocessor_config.json  \n",
      "  inflating: output_checkpoint/checkpoint-7325/adapter_config.json  \n",
      "  inflating: output_checkpoint/checkpoint-7325/adapter_model.safetensors  \n",
      "  inflating: output_checkpoint/checkpoint-7325/README.md  \n",
      "  inflating: output_checkpoint/checkpoint-7300/trainer_state.json  \n",
      "  inflating: output_checkpoint/checkpoint-7300/rng_state.pth  \n",
      "  inflating: output_checkpoint/checkpoint-7300/scheduler.pt  \n",
      "  inflating: output_checkpoint/checkpoint-7300/optimizer.pt  \n",
      "  inflating: output_checkpoint/checkpoint-7300/training_args.bin  \n",
      "  inflating: output_checkpoint/checkpoint-7300/processor_config.json  \n",
      "  inflating: output_checkpoint/checkpoint-7300/chat_template.json  \n",
      "  inflating: output_checkpoint/checkpoint-7300/tokenizer.json  \n",
      "  inflating: output_checkpoint/checkpoint-7300/tokenizer.model  \n",
      "  inflating: output_checkpoint/checkpoint-7300/added_tokens.json  \n",
      "  inflating: output_checkpoint/checkpoint-7300/special_tokens_map.json  \n",
      "  inflating: output_checkpoint/checkpoint-7300/tokenizer_config.json  \n",
      "  inflating: output_checkpoint/checkpoint-7300/preprocessor_config.json  \n",
      "  inflating: output_checkpoint/checkpoint-7300/adapter_config.json  \n",
      "  inflating: output_checkpoint/checkpoint-7300/adapter_model.safetensors  \n",
      "  inflating: output_checkpoint/checkpoint-7300/README.md  \n"
     ]
    }
   ],
   "source": [
    "# unzip the output checkpoints!\n",
    "!unzip \"output_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccada99-b8e4-4a72-ac08-6e3fab611070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start training, checkpoints will start auto compiling\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "316fa87f-49aa-40eb-90c8-cc5d785d4e63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 176,236 | Num Epochs = 1 | Total steps = 7,343\n",
      "O^O/ \\_/ \\    Batch size per device = 6 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (6 x 4 x 1) = 24\n",
      " \"-____-\"     Trainable parameters = 14,901,248/4,000,000,000 (0.37% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7343' max='7343' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7343/7343 : < :, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # For resuming if stoped midway\n",
    "# trainer_stats = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1769621-321f-49e5-ab8d-ff7283f60529",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1a7aa53-f16a-4f2d-9179-9256bcdf6c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gemma-3coder_loraweights/processor_config.json']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving model LORA weights only, not the full model!\n",
    "model.save_pretrained(\"gemma-3coder_loraweights\")  # Local saving\n",
    "tokenizer.save_pretrained(\"gemma-3coder_loraweights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f42c064-311f-4ed8-a41a-474c93c4fcc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7760c9ff58254c1f886ed734366932e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df88458e9a1d43cb97b88c7bff218c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4280cd965aa44d2a9a61d14b29653a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/59.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/hiteshmodi/unsloth_gemma-3coder_loraweights\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbdbb9f7a3974696813f0271c5045a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f34bec7f20481c9337a28a9811b655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8db71319d444166ba400f82a217f490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Push to Hugging Face (private repo), only the LORA weights\n",
    "trainer.model.push_to_hub(\"hiteshmodi/unsloth_gemma-3coder_loraweights\", private=True)\n",
    "trainer.tokenizer.push_to_hub(\"hiteshmodi/unsloth_gemma-3coder_loraweights\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1afaf5-bf2e-416c-b744-c4c6ab05a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will convert the model into binary that Ollama can use without requiring pytorch or transformers, its gguf format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9f5ae5b-26e9-40d4-b348-79cff260dcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00002.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Downloading safetensors index for unsloth/gemma-3-4b-it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2173e8ae794701bc22fce4df5fd646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfd218fcc6c4d8cbaea3ab2d26483cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:38<00:38, 38.07s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a7cac922104fc5b48bb617b002b9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:04<00:00, 32.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Updating system package directories\n",
      "Unsloth: Install GGUF and other packages\n",
      "Unsloth GGUF:hf-to-gguf:Loading model: gemma-3-finetune_coder\n",
      "Unsloth GGUF:hf-to-gguf:Model architecture: Gemma3ForConditionalGeneration\n",
      "Unsloth GGUF:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "Unsloth GGUF:hf-to-gguf:Exporting model...\n",
      "Unsloth GGUF:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\n",
      "Unsloth GGUF:hf-to-gguf:token_embd.weight,                 torch.bfloat16 --> Q8_0, shape = {2560, 262208}\n",
      "Unsloth GGUF:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\n",
      "Unsloth GGUF:hf-to-gguf:output_norm.weight,                torch.bfloat16 --> F32, shape = {2560}\n",
      "Unsloth GGUF:hf-to-gguf:Set meta model\n",
      "Unsloth GGUF:hf-to-gguf:Set model parameters\n",
      "Unsloth GGUF:hf-to-gguf:Set model quantization version\n",
      "Unsloth GGUF:hf-to-gguf:Set model tokenizer\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type bos to 2\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type eos to 106\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type unk to 3\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type pad to 0\n",
      "Unsloth GGUF:gguf.vocab:Setting add_bos_token to True\n",
      "Unsloth GGUF:gguf.vocab:Setting add_eos_token to False\n",
      "Unsloth GGUF:gguf.vocab:Setting chat_template to {{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "..... Chat template truncated .....\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414a6108f1f84e96ac3e451beb14f306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: GGUF conversion:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth GGUF:hf-to-gguf:Model successfully exported to ./\n",
      "Unsloth: Converted to gemma-3-finetune_coder.Q8_0.gguf with size = 4.1G\n",
      "Unsloth: Successfully saved GGUF to:\n",
      "gemma-3-finetune_coder.Q8_0.gguf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gemma-3-finetune_coder.Q8_0.gguf']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying to use solution we got from github issues section for unloading full model, use model merge then save pretrained.\n",
    "# I used an additional save_pretrained_merged function call as a workaround. It creates the config.json correctly, and the subsequent gguf creation goes through.\n",
    "\n",
    "model.save_pretrained_merged(\"gemma-3-finetune_coder\", tokenizer)\n",
    "\n",
    "model.save_pretrained_gguf(\n",
    "\"gemma-3-finetune_coder\",\n",
    "quantization_type = \"Q8_0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dea9e6d9-3ba0-487a-a97c-5e9e88131421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-rw- 1 root root 3.9G Jun  8 06:23 gemma-3-finetune_coder.Q8_0.gguf\n"
     ]
    }
   ],
   "source": [
    "# its a 3.9G file, this is the full model that can be used with Ollama now\n",
    "ls -lh gemma-3-finetune_coder.Q8_0.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73038847-9f89-41b5-96ed-cb2d2f0a77d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d73cca6ede48989f415358e087ebaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f827e194678446d9dc972be6f51bedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85bf07fafc2a427b9fdf5a6195efbfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/hiteshmodi/unsloth_gemma3coderfull-4b-it\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3325291ff6af41c38bb703fa9b529f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb81264cd5c741a2a74ee8c05cced003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d74f96b5e04d94b9ddc2c2c182067f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Push to Hugging Face (private repo), only the LORA weights\n",
    "trainer.model.push_to_hub(\"hiteshmodi/unsloth_gemma3coderfull-4b-it\", private=True)\n",
    "trainer.tokenizer.push_to_hub(\"hiteshmodi/unsloth_gemma3coderfull-4b-it\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d74839e4-2126-4bd6-a24e-dbe9e8724869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moules required for uploading thi sto huggingface\n",
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa3ed57c-76c7-46c7-9ead-b6151e64b0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a13b6731184f538d9e7d715b38b87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646882fc216f44a9b3d5ca163139db12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma-3-finetune_coder.Q8_0.gguf:   0%|          | 0.00/4.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/hiteshmodi/unsloth_gemma3coderfull-4b-it/commit/dc71196d905d7aa5849590116b1022d56fa0e618', commit_message='Upload gemma-3-finetune_coder.Q8_0.gguf with huggingface_hub', commit_description='', oid='dc71196d905d7aa5849590116b1022d56fa0e618', pr_url=None, repo_url=RepoUrl('https://huggingface.co/hiteshmodi/unsloth_gemma3coderfull-4b-it', endpoint='https://huggingface.co', repo_type='model', repo_id='hiteshmodi/unsloth_gemma3coderfull-4b-it'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"gemma-3-finetune_coder.Q8_0.gguf\",\n",
    "    path_in_repo=\"gemma-3-finetune_coder.Q8_0.gguf\",\n",
    "    repo_id=\"hiteshmodi/unsloth_gemma3coderfull-4b-it\",\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6efd0504-bfa8-4fd9-bfc4-34a347f23642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5aaadd95e0842a2aa9c82fff7c44581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989578af24cc4d5ba6811927d75f6acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "output_checkpoint.zip:   0%|          | 0.00/274M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/hiteshmodi/unsloth_gemma3coderfull-4b-it/commit/92f220ba213da70db9789c99555006364352fb3e', commit_message='Upload output_checkpoint.zip with huggingface_hub', commit_description='', oid='92f220ba213da70db9789c99555006364352fb3e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/hiteshmodi/unsloth_gemma3coderfull-4b-it', endpoint='https://huggingface.co', repo_type='model', repo_id='hiteshmodi/unsloth_gemma3coderfull-4b-it'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets upload the output checkpoints as well!\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"output_checkpoint.zip\",\n",
    "    path_in_repo=\"output_checkpoint.zip\",\n",
    "    repo_id=\"hiteshmodi/unsloth_gemma3coderfull-4b-it\",\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c72f11-3edd-4f55-ab65-0894b1acdc19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
