{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f466d89-a76b-4d9a-91f9-1eef52261ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be used with runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bcd87c3-9d82-4a07-880d-6bfec6645a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each chekcpoint in this notebook is 130 mb, old are not deleted when new are here, calculate required storage as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e0f2016-531c-4e73-8631-095de59bd00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDO NOT!\\nsave output checkout folder name as checkpoint. errors ayega\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DO NOT!\n",
    "save output checkout folder name as checkpoint. errors ayega,\n",
    "\n",
    "No worries, we have save steps :) ~ this only keeps x newest backups,\n",
    "\n",
    "-> if latest directory is corrupted! delet it and it will be fine!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9960d1bf-e47f-4a31-ae89-263c396362d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e51bd55-0f97-4c29-8c62-5be93db793b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing stuff, onl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b8da1d-2476-42c7-a7e2-1176e79831cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall -y unsloth unsloth_zoo torch xformers transformers triton protobuf wheel peft trl accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b38e80c-5280-4590-8723-f9e3d6c4419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First: install correct torch\n",
    "!pip install torch==2.6.0 torchvision\n",
    "\n",
    "# Then: install remaining tools with pinned versions\n",
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "\n",
    "# Fix any known dependency mismatches\n",
    "!pip install \\\n",
    "    \"protobuf<4.0.0\" \\\n",
    "    \"wheel>=0.42.0\" \\\n",
    "    sentencepiece \\\n",
    "    \"datasets>=3.4.1\" \\\n",
    "    huggingface_hub \\\n",
    "    hf_transfer \\\n",
    "    transformers==4.51.3 \\\n",
    "    safetensors \\\n",
    "    msgspec \\\n",
    "    tyro \\\n",
    "    regex \\\n",
    "    rich\n",
    "\n",
    "# Finally: install Unsloth itself\n",
    "!pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "857b1cf7-bd83-4b5a-822d-a6d23a59b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Unsloth Models\n",
    "from unsloth import FastModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2d4e7d-c5a0-4dfb-86a8-0d879970e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing other required models\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0411dfe6-9314-48c6-b803-9205d5aa5579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging into hugigngface as some of our datasets are gated!\n",
    "from huggingface_hub import login\n",
    "secret_value_0 = \"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\"\n",
    "login(token=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "94841dff-f688-4aa8-9d8a-19cd045c040e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.1: Fast Gemma3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA A40. Num GPUs = 1. Max memory: 44.448 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# Preparing our model\n",
    "max_seq_length_ = 4096\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "\n",
    "    # Other popular models!\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/Llama-3.3-70B\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
    "    \"unsloth/Phi-4\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it\",\n",
    "    max_seq_length = max_seq_length_, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6f56193-acc1-45e2-9ee5-e99afa1f9585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.language_model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "# Loading model base\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 8,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 8,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1264e139-7979-4a42-a731-3d5acd9e23b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Download from huggingface\n",
    "# 1. CodeAnalyst\n",
    "python_dataset = load_dataset(\"flytech/python-codes-25k\")\n",
    "# 2. Text-to-SQL\n",
    "sql_dataset = load_dataset(\"gretelai/synthetic_text_to_sql\")\n",
    "# 3. PySpark QA\n",
    "pyspark_dataset = load_dataset(\"irfanalisoomro/clean_generated_pyspark_answers\")\n",
    "# 4. Leetcode\n",
    "# leetcode_dataset = load_dataset(\"DenCT/leetcode-python-solutions-with-exaplanations\")\n",
    "leetcode_dataset = load_dataset(\"LimYeri/LeetCode_Python_Solutions_v2\")\n",
    "# 5. Neetcode\n",
    "neetcode_dataset = load_dataset(\"nischalon10/neetcode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a1e2360-31c6-493e-ae6e-5f9e2254089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample conversion function for each dataset type\n",
    "def convert_to_gemma_format(user_text, model_text):\n",
    "    return f\"<start_of_turn>user\\n{user_text}<end_of_turn>\\n<start_of_turn>model\\n{model_text}<end_of_turn>\"\n",
    "\n",
    "# --- Load your real datasets below instead of these small examples ---\n",
    "\n",
    "def format_python_dataset(example):\n",
    "    prompt = f\"{example['instruction']}\\n{example['input']}\".strip()\n",
    "    return {\"text\": convert_to_gemma_format(prompt, example[\"output\"])}\n",
    "\n",
    "# 2. Text-to-SQL dataset\n",
    "def format_sql_dataset(example):\n",
    "    prompt = f\"{example['sql_prompt']}\\n{example['sql_context']}\".strip()\n",
    "    model_reply = f\"{example['sql']}\\n\\nExplanation:\\n{example['sql_explanation']}\"\n",
    "    return {\"text\": convert_to_gemma_format(prompt, model_reply)}\n",
    "\n",
    "# 3. PySpark Q&A dataset\n",
    "def format_pyspark_dataset(example):\n",
    "    return {\"text\": convert_to_gemma_format(example[\"Question\"], example[\"Answer\"])}\n",
    "\n",
    "# 4. Leetcode solutions dataset\n",
    "# def format_leetcode_dataset(example):\n",
    "#     return {\"text\": convert_to_gemma_format(example[\"question_content\"], example[\"content\"])}\n",
    "\n",
    "def format_leetcode_dataset(example):\n",
    "    return {\"text\": convert_to_gemma_format(example[\"question_content\"], example[\"content\"])}\n",
    "\n",
    "def format_neetcode_dataset(example):\n",
    "    return {\"text\": convert_to_gemma_format(example[\"content\"], example[\"python\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "26612c31-8eff-47cd-82d1-de524ca9b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map your formatting functions on each dataset's \"train\" split, removing original columns.\n",
    "formatted_python = python_dataset[\"train\"].map(\n",
    "    format_python_dataset, \n",
    "    remove_columns=python_dataset[\"train\"].column_names\n",
    ")\n",
    "formatted_sql = sql_dataset[\"train\"].map(\n",
    "    format_sql_dataset, \n",
    "    remove_columns=sql_dataset[\"train\"].column_names\n",
    ")\n",
    "formatted_pyspark = pyspark_dataset[\"train\"].map(\n",
    "    format_pyspark_dataset, \n",
    "    remove_columns=pyspark_dataset[\"train\"].column_names\n",
    ")\n",
    "formatted_leetcode = leetcode_dataset[\"train\"].map(\n",
    "    format_leetcode_dataset, \n",
    "    remove_columns=leetcode_dataset[\"train\"].column_names\n",
    ")\n",
    "formatted_neetcode = neetcode_dataset[\"train\"].map(\n",
    "    format_neetcode_dataset, \n",
    "    remove_columns=neetcode_dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dcea9dd0-b1f6-4499-a881-d8f89247822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all formatted results into one Dataset containing only the \"text\" field.\n",
    "combined_texts = (\n",
    "    formatted_python[\"text\"] +\n",
    "    formatted_sql[\"text\"] +\n",
    "    formatted_pyspark[\"text\"] +\n",
    "    formatted_leetcode[\"text\"] +\n",
    "    formatted_neetcode[\"text\"]\n",
    ")\n",
    "gemma_dataset = Dataset.from_dict({\"text\": combined_texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0fa36e87-b07b-4bd9-8c4f-ed18a95a1adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 176236\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fb82ea24-4370-458a-8f6a-184e43e2eab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d93a6f964ae48e0a83c9e775c9510cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/176236 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training the model with custom settings\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = gemma_dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 6,\n",
    "        gradient_accumulation_steps = 4,    # Use GA to mimic batch size!\n",
    "        output_dir = \"output_checkpoint\",   #Necessary, if you want to save checkpoint and later continue\n",
    "        save_strategy = \"steps\",            #Necessary, if you want to save checkpoint and later continue\n",
    "        save_steps = 25,                     #Necessary, if you want to save checkpoint and later continue\n",
    "        save_total_limit=3,                 #Only save latest 2 checkpoints!\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        # max_steps = 30,\n",
    "        learning_rate = 2e-5, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        dataset_num_proc=2,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b4ea65d3-bdbc-4594-9aa9-6ea7267aeff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649b27fea87149a09a316dd7dc2a9443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=96):   0%|          | 0/176236 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Next piece of training!\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04d10ba5-c7b0-48d8-8de0-56201c32737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id recommend using the refined dataset instead, config changes cause us issues later!\n",
    "\"\"\"\n",
    "def is_valid(example):\n",
    "    try:\n",
    "        _ = tokenizer(example[\"text\"])\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "filtered_dataset = dataset.filter(is_valid, num_proc=2)\n",
    "\"\"\"\n",
    "\n",
    "# ignore_index is a pyhton pandas command i believe, it causes issues in model and stops execution, so we have to add this piece or filte rout all training data with it\n",
    "if not hasattr(model.config, \"ignore_index\"):\n",
    "    model.config.ignore_index = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccada99-b8e4-4a72-ac08-6e3fab611070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start training, checkpoints will start auto compiling\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "316fa87f-49aa-40eb-90c8-cc5d785d4e63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 176,236 | Num Epochs = 1 | Total steps = 7,343\n",
      "O^O/ \\_/ \\    Batch size per device = 6 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (6 x 4 x 1) = 24\n",
      " \"-____-\"     Trainable parameters = 14,901,248/4,000,000,000 (0.37% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7343' max='7343' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7343/7343 : < :, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For resuming if stoped midway\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "616dee19-275e-488f-b436-8b0f6ec00ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['final_model/processor_config.json']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "trainer.model.save_pretrained(\"final_model\")\n",
    "trainer.tokenizer.save_pretrained(\"final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6757aac-4766-4ed9-9266-d3c511d0d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging in via upload token\n",
    "secret_value_0 = \"<enterhere>\"\n",
    "login(token=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1dc41a33-ee24-47b7-aee9-6d6910ee3fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ba8bf8a54e4d90b3131126645d6122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba5b1782d9b4e5ba1f6d66c2dbbd37e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435d8119758f4b258094c88497ba3b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/59.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/hiteshmodi/unsloth_gemma-3coder-4b-it\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d0affdf59941a29041c6c66915c8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365fcff8def247d0b28e3380c3136086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9655510b0d4c41994eb54c45c7848c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Push to Hugging Face (private repo)\n",
    "trainer.model.push_to_hub(\"hiteshmodi/unsloth_gemma-3coder-4b-it\", private=True)\n",
    "trainer.tokenizer.push_to_hub(\"hiteshmodi/unsloth_gemma-3coder-4b-it\", private=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "When you fine-tune models using Unsloth or other LoRA-based methods:\n",
    "\n",
    "You’re not modifying the full base model weights (which are ~4–5GB for 4B models).\n",
    "\n",
    "Instead, you’re training and saving just the LoRA adapter weights, which are much smaller (often 30–100 MB).\n",
    "\n",
    "To use the model:\n",
    "\n",
    "Hugging Face expects that you already have the base Gemma 3 model locally or available from Hugging Face, and it will:\n",
    "\n",
    "Load the base model.\n",
    "\n",
    "Apply the LoRA weights on top of it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8550cec-f6a9-4d0c-a573-5d698ddb1a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is also saving ht emodel weights only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d1a7aa53-f16a-4f2d-9179-9256bcdf6c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gemma-3coder/processor_config.json']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"gemma-3coder\")  # Local saving\n",
    "tokenizer.save_pretrained(\"gemma-3coder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f42c064-311f-4ed8-a41a-474c93c4fcc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7a6c74be864f5cbfcd6299f2578466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1a371c1a224b1883b863da5803933a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78808b1191a143598e3e8a974783c180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/59.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/hiteshmodi/unsloth_gemma-3coder2-4b-it\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9b3008de344dbdb56c58d993d2857a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40c986e062f437f8f0068181f3aebe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c2396158e847089b4114ade64ccccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Push to Hugging Face (private repo)\n",
    "trainer.model.push_to_hub(\"hiteshmodi/unsloth_gemma-3coder2-4b-it\", private=True)\n",
    "trainer.tokenizer.push_to_hub(\"hiteshmodi/unsloth_gemma-3coder2-4b-it\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1afaf5-bf2e-416c-b744-c4c6ab05a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now this will convert the model into binary that Ollama can use without requiring pytorch or transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65447602-f361-44b1-b3a8-9667d3f250d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we had modified the config file above, We have to do this now as it was not saved. Id recommend to not touch config file instead impleemnt the clean fucntion\n",
    "\"\"\"\n",
    "def is_valid(example):\n",
    "    try:\n",
    "        _ = tokenizer(example[\"text\"])\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "filtered_dataset = dataset.filter(is_valid, num_proc=2)\n",
    "\"\"\"\n",
    "\n",
    "model.config.ignore_index = -100  # (Again just to be safe)\n",
    "model.config.save_pretrained(\"gemma-3coder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2a9eb2ca-d9b8-4c41-879d-61a69d15f027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth GGUF:hf-to-gguf:Loading model: gemma-3coder\n",
      "Unsloth GGUF:hf-to-gguf:Model architecture: Gemma3ForConditionalGeneration\n",
      "Unsloth GGUF:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "Unsloth GGUF:hf-to-gguf:Exporting model...\n",
      "Unsloth GGUF:hf-to-gguf:Set meta model\n",
      "Unsloth GGUF:hf-to-gguf:Set model parameters\n",
      "Unsloth GGUF:hf-to-gguf:Set model quantization version\n",
      "Unsloth GGUF:hf-to-gguf:Set model tokenizer\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type bos to 2\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type eos to 106\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type unk to 3\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type pad to 0\n",
      "Unsloth GGUF:gguf.vocab:Setting add_bos_token to True\n",
      "Unsloth GGUF:gguf.vocab:Setting add_eos_token to False\n",
      "Unsloth GGUF:gguf.vocab:Setting chat_template to {{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "..... Chat template truncated .....\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99fa9f3689984b7b9a7fc274673c53b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: GGUF conversion:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth GGUF:hf-to-gguf:Model successfully exported to ./\n",
      "Unsloth GGUF:hf-to-gguf:Loading model: gemma-3coder\n",
      "Unsloth GGUF:hf-to-gguf:Model architecture: Gemma3ForConditionalGeneration\n",
      "Unsloth GGUF:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "Unsloth GGUF:hf-to-gguf:Exporting model...\n",
      "Unsloth GGUF:hf-to-gguf:Set meta model\n",
      "Unsloth GGUF:hf-to-gguf:Set model parameters\n",
      "Unsloth GGUF:hf-to-gguf:Set model quantization version\n",
      "Unsloth GGUF:hf-to-gguf:Set model tokenizer\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type bos to 2\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type eos to 106\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type unk to 3\n",
      "Unsloth GGUF:gguf.vocab:Setting special token type pad to 0\n",
      "Unsloth GGUF:gguf.vocab:Setting add_bos_token to True\n",
      "Unsloth GGUF:gguf.vocab:Setting add_eos_token to False\n",
      "Unsloth GGUF:gguf.vocab:Setting chat_template to {{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "..... Chat template truncated .....\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e1389d559745729a4aed37572bd537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: GGUF conversion:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth GGUF:hf-to-gguf:Model successfully exported to ./\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsloth: Failed to convert llama.cpp/unsloth_convert_hf_to_gguf.py to GGUF.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained_gguf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemma-3coder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mF16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# or F16/BF16 if your hardware supports it\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/save.py:2255\u001b[0m, in \u001b[0;36msave_to_gguf_generic\u001b[0;34m(model, save_directory, quantization_type, repo_id, token)\u001b[0m\n\u001b[1;32m   2252\u001b[0m     install_llama_cpp(just_clone_repo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2253\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_convert_to_gguf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mquantization_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2261\u001b[0m     prepare_saving(\n\u001b[1;32m   2262\u001b[0m         model,\n\u001b[1;32m   2263\u001b[0m         repo_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         token \u001b[38;5;241m=\u001b[39m token,\n\u001b[1;32m   2268\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth_zoo/llama_cpp.py:692\u001b[0m, in \u001b[0;36mconvert_to_gguf\u001b[0;34m(input_folder, output_filename, quantization_type, max_shard_size, print_output, print_outputs)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Failed to convert \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to GGUF.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    694\u001b[0m printed_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(metadata)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_output: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Successfully saved GGUF to:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprinted_metadata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsloth: Failed to convert llama.cpp/unsloth_convert_hf_to_gguf.py to GGUF."
     ]
    }
   ],
   "source": [
    "model.save_pretrained_gguf(\n",
    "    \"gemma-3coder\",\n",
    "    quantization_type = \"F16\",  # or F16/BF16 if your hardware supports it\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f5ae5b-26e9-40d4-b348-79cff260dcff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
